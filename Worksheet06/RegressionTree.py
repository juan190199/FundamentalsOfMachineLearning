import numpy as npclass Node:    passclass Tree:    def __init__(self):        self.root = Node()    def find_leaf(self, x):        node = self.root        while hasattr(node, "feature"):            j = node.feature            if x[j] <= node.threshold:                node = node.left            else:                node = node.right        return nodeclass RegressionTree(Tree):    def __init__(self):        super(RegressionTree, self).__init__()    def fit(self, data, labels, n_min=200):        """        :param data: the feature matrix for all digits        :param labels: the corresponding ground-truth responses        :param n_min: termination criterion (don't split if a node contains fewer instances)        :return:        """        N, D = data.shape        D_try = int(np.sqrt(D))  # how many features to consider for each split decision        # initialize the root node        self.root.data = data        self.root.labels = labels        # build the tree        # put root in stack        stack = [self.root]        while len(stack):            node = stack.pop()            n = node.data.shape[0]  # number of instances in present node            if n >= n_min:                # Call 'make_decision_split_node()' with 'D_try' randomly selected                # feature indices. This turns 'node' into a split node                # and returns the two children, which must be placed on the 'stack'.                perm = np.random.permutation(D)  # permute D indices                left, right = make_regression_split_node(node, perm[:D_try])  # select :D_try of permuted indices                # for 'make_regression_split_node()'                # put children in stack                stack.append(left)                stack.append(right)            else:                # Call 'make_regression_leaf_node()' to turn 'node' into a leaf node.                make_regression_leaf_node(node)    def predict(self, dset):        pred_set = np.apply_along_axis(self.predict_single, axis=1, arr=dset)        return pred_set    def predict_single(self, x):        leaf = self.find_leaf(x)        # compute p(y | x)        return leaf.responsedef make_regression_split_node(node, feature_indices):    """    :param node: the node to be split    :param feature_indices: a numpy array of length 'D_try',    containing the feature indices to be considered in the present split    :return:    """    n, D = node.data.shape    # find best feature j (among 'feature_indices') and best threshold t for the split    e_min = 1e100    j_min, t_min = 0, 0    for j in feature_indices:        # remove duplicate features        dj = np.sort(np.unique(node.data[:, j]))        # compute candidate thresholds in the middle between consecutive feature values        tj = 0.5 * (dj[1:] + dj[:-1])        # each candidate threshold we need to compute Gini impurities of the resulting children node        for t in tj:            left_indices = node.data[:, j] <= t            nl = np.sum(left_indices)            ll = node.labels[left_indices]            # el = nl * (1 - np.sum(np.square(np.bincount(ll)/nl)))            el = ((ll - ll.mean()) ** 2).sum()            nr = n - nl            lr = node.labels[node.data[:, j] > t]            # er = nr * (1 - np.sum(np.square(np.bincount(lr)/nr)))            er = ((lr - lr.mean()) ** 2).sum()            # choose the the best threshold that minimizes sum of Gini impurities            if el + er < e_min:                e_min = el + er                j_min = j                t_min = t    # create children    left = Node()    right = Node()    # initialize 'left' and 'right' with the data subsets and labels    # according to the optimal split found above    left.data = node.data[node.data[:, j_min] <= t_min, :]    left.labels = node.labels[node.data[:, j_min] <= t_min]    right.data = node.data[node.data[:, j_min] > t_min, :]    right.labels = node.labels[node.data[:, j_min] > t_min]    # turn the current 'node' into a split node    # (store children and split condition)    node.left = left    node.right = right    node.feature = j_min    node.threshold = t_min    # return the children (to be placed on the stack)    return left, rightdef make_regression_leaf_node(node):    """    :param node: the node to become a leaf    :return:    """    node.N = node.labels.shape[0]    # node.response = np.bincount(node.labels, minlength=10) / node.N    node.response = node.labels.mean()